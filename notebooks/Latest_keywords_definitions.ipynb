{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF -> Embeddings -> Azure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Azure Cognitive Search SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --index-url=https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/ azure-search-documents==11.4.0a20230509004\n",
    "# %pip install azure-identity\n",
    "# %pip install langchain===0.0.200\n",
    "# %pip install openai, tiktoken\n",
    "# %pip install pypdf\n",
    "# %pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-identity in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from azure-identity) (1.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from azure-identity) (1.12.0)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.20.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from azure-identity) (1.22.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from azure-identity) (3.4.8)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.11.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from azure-identity) (1.27.1)\n",
      "Requirement already satisfied: requests>=2.18.4 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from azure-core<2.0.0,>=1.11.0->azure-identity) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from azure-core<2.0.0,>=1.11.0->azure-identity) (4.5.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from cryptography>=2.5->azure-identity) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.21)\n",
      "Requirement already satisfied: PyJWT[crypto]<3,>=1.0.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from msal<2.0.0,>=1.20.0->azure-identity) (2.1.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.6 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity) (2.7.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\divya.koyyalamudi\\appdata\\roaming\\python\\python39\\site-packages (from portalocker<3,>=1.6->msal-extensions<2.0.0,>=0.3.0->azure-identity) (304)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (0.0.200)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from langchain) (1.10.10)\n",
      "Requirement already satisfied: langchainplus-sdk>=0.0.9 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from langchain) (0.0.19)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from langchain) (4.0.1)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from langchain) (2.27.1)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from langchain) (1.21.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from langchain) (1.4.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (5.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from async-timeout<5.0.0,>=4.0.0->langchain) (4.5.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2021.10.8)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: 'openai,'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from pypdf) (4.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-identity\n",
    "%pip install langchain\n",
    "%pip install openai, tiktoken\n",
    "%pip install pypdf\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (0.27.4)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from openai) (2.27.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from openai) (4.64.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from aiohttp->openai) (5.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from aiohttp->openai) (21.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->openai) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.4)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (0.3.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from tiktoken) (2.27.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from tiktoken) (2022.3.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\divya.koyyalamudi\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ RUN THIS ############\n",
    "import os, json\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.schema import BaseRetriever\n",
    "from langchain.vectorstores.azuresearch import AzureSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure OpenAI settings\n",
    "Configure the OpenAI settings to use Azure OpenAI or OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ RUN THIS ############\n",
    "\n",
    "# Load environment variables from a .env file using load_dotenv():\n",
    "load_dotenv()\n",
    "\n",
    "azure_openai_api_key: str = os.environ.get('AZURE_OPENAI_API_KEY')\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://verx-corp-ai.openai.azure.com/\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "openai.api_key = azure_openai_api_key\n",
    "openai.openai_api_key = azure_openai_api_key\n",
    "model: str = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure vector store settings\n",
    " \n",
    "Set up the vector store settings using environment variables:\n",
    "Change the index name to be your index...  the index name here will also be used for the similarity search below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ RUN THIS ############\n",
    "\n",
    "vector_store_address: str = 'https://violet-vector.search.windows.net'\n",
    "vector_store_password: str = os.environ.get('AZURE_VECTOR_STORE_PASSWORD')\n",
    "index_name: str = \"et-dossier-fuel-test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings and vector store instances\n",
    " \n",
    "Create instances of the OpenAIEmbeddings and AzureSearch classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ RUN THIS ############\n",
    "\n",
    "embeddings: OpenAIEmbeddings = OpenAIEmbeddings(model=model, chunk_size=1, openai_api_key=azure_openai_api_key, deployment='VERX-CORP-ADA-002')  \n",
    "vector_store: AzureSearch = AzureSearch(azure_search_endpoint=vector_store_address,  \n",
    "                                        azure_search_key=vector_store_password,  \n",
    "                                        index_name=index_name,  \n",
    "                                        embedding_function=embeddings.embed_query)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from PDF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "862"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"../data/fuel-docs/\")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "859"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split the docs into smaller chunks of desired size\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=8000,chunk_overlap=200)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "len(split_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic model(OpenAI chat completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "vjson = open('C:/Users/Divya.Koyyalamudi/Downloads/Azure Open AI Deployment.postman_collection 1.json')\n",
    "json_data = json.load(vjson)\n",
    "header_data = {json_data['item'][0]['request']['header'][0]['key'] : json_data['item'][0]['request']['header'][0]['value'] }\n",
    "body_data = json.loads(json_data['item'][0]['request']['body']['raw'])\n",
    "def generic_model(text, template,temperature, max_tokens ):\n",
    "    \"\"\"Generic gpt4 model\n",
    "    \"\"\"\n",
    "  \n",
    "    body_data['temperature'] = temperature\n",
    "    body_data['max_tokens'] = max_tokens\n",
    "    runner = None\n",
    "    final_str = str()\n",
    "    body_data['messages'][0]['content'] = template  + text   \n",
    "    for i in range(0,1):\n",
    "            body_data['messages'][0]['content'] = template + text      \n",
    "            try:\n",
    "                resp = requests.post('https://verx-corp-ai.openai.azure.com/openai/deployments/VERX-CORP-GPT40/chat/completions?api-version=2023-03-15-preview'\n",
    "                        , headers = header_data, json = body_data)\n",
    "                print(resp.status_code)\n",
    "                #print(resp.json())\n",
    "                final_str = resp.json()['choices'][0]['message']['content'] \n",
    "                \n",
    "            except Exception as e:\n",
    "                print('ERRORED OUT :(\\n\\n', e) \n",
    "                time.sleep(60) #35\n",
    "                print('retrying...')\n",
    "                try:\n",
    "                    resp = requests.post('https://verx-corp-ai.openai.azure.com/openai/deployments/VERX-CORP-GPT40/chat/completions?api-version=2023-03-15-preview'\n",
    "                        , headers = header_data, json = body_data)\n",
    "                    print(resp.status_code)\n",
    "                    final_str = resp.json()['choices'][0]['message']['content']   \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    \n",
    "                \n",
    "    print('Keywords generation successful!!')\n",
    "    return final_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(model_name= 'gpt-4' )\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the no. of tokens in text string\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_max_tokens(text, template):\n",
    "    \"\"\"Input max_tokens\n",
    "    \"\"\"\n",
    "    tokens_text = num_tokens_from_string(text,'cl100k_base')\n",
    "    \n",
    "   \n",
    "    tokens_max_template = len(template) #len is biggest for a prompt considering that token\n",
    "    print('tokens_text:' ,tokens_text,  'tokens_max_template:',tokens_max_template,)\n",
    "    return int(32000- int(tokens_text) - int(tokens_max_template))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,openai\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from a .env file using load_dotenv():\n",
    "load_dotenv()\n",
    "\n",
    "azure_openai_api_key: str = os.environ.get('AZURE_OPENAI_API_KEY')\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://verx-corp-ai.openai.azure.com/\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "openai.api_key = azure_openai_api_key\n",
    "openai.openai_api_key = azure_openai_api_key\n",
    "deployment_id: str = \"VERX-CORP-DAVINCI\"\n",
    "model: str = \"text-davinci-003\"\n",
    "\n",
    "from langchain import OpenAI \n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(temperature=0, model=model, openai_api_key=azure_openai_api_key, deployment_id=deployment_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap\n",
    "\n",
    "#customize the prompt\n",
    "\n",
    "map_prompt_template =  \"\"\"You are a research analyst. I will provide you with a section of a document and you will create a list of most frquently used keywords.\n",
    "\n",
    "Input: {text} \"\"\"\n",
    "\n",
    "combine_prompt_template = \"\"\"You are a copy editor. Combine the below keywords to a single list. \n",
    "\n",
    "Input: {text} \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the keywords using langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "105\n",
      "110\n",
      "115\n",
      "120\n",
      "125\n",
      "130\n",
      "135\n",
      "140\n",
      "145\n",
      "150\n",
      "155\n",
      "160\n",
      "165\n",
      "170\n",
      "175\n",
      "180\n",
      "185\n",
      "190\n",
      "195\n",
      "200\n",
      "205\n",
      "210\n",
      "215\n",
      "220\n",
      "225\n",
      "230\n",
      "235\n",
      "240\n",
      "245\n",
      "250\n",
      "255\n",
      "260\n",
      "265\n",
      "270\n",
      "275\n",
      "280\n",
      "285\n",
      "290\n",
      "295\n",
      "300\n",
      "305\n",
      "310\n",
      "315\n",
      "320\n",
      "325\n",
      "330\n",
      "335\n",
      "340\n",
      "345\n",
      "350\n",
      "355\n",
      "360\n",
      "365\n",
      "370\n",
      "375\n",
      "380\n",
      "385\n",
      "390\n",
      "395\n",
      "400\n",
      "405\n",
      "410\n",
      "415\n",
      "420\n",
      "425\n",
      "430\n",
      "435\n",
      "440\n",
      "445\n",
      "450\n",
      "455\n",
      "460\n",
      "465\n",
      "470\n",
      "475\n",
      "480\n",
      "485\n",
      "490\n",
      "495\n",
      "500\n",
      "505\n",
      "510\n",
      "515\n",
      "520\n",
      "525\n",
      "530\n",
      "535\n",
      "540\n",
      "545\n",
      "550\n",
      "555\n",
      "560\n",
      "565\n",
      "570\n",
      "575\n",
      "580\n",
      "585\n",
      "590\n",
      "595\n",
      "600\n",
      "605\n",
      "610\n",
      "615\n",
      "620\n",
      "625\n",
      "630\n",
      "635\n",
      "640\n",
      "645\n",
      "650\n",
      "655\n",
      "660\n",
      "665\n",
      "670\n",
      "675\n",
      "680\n",
      "685\n",
      "690\n",
      "695\n",
      "700\n",
      "705\n",
      "710\n",
      "715\n",
      "720\n",
      "725\n",
      "730\n",
      "735\n",
      "740\n",
      "745\n",
      "750\n",
      "755\n",
      "760\n",
      "765\n",
      "770\n",
      "775\n",
      "780\n",
      "785\n",
      "790\n",
      "795\n",
      "800\n",
      "805\n",
      "810\n",
      "815\n",
      "820\n",
      "825\n",
      "830\n",
      "835\n",
      "840\n",
      "845\n",
      "850\n",
      "855\n"
     ]
    }
   ],
   "source": [
    "\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "keyword=[]\n",
    "chain = load_summarize_chain(llm, chain_type='map_reduce', map_prompt=map_prompt, combine_prompt=combine_prompt, verbose=False)\n",
    "for i in range(0,len(split_docs),5):\n",
    "    print(i)\n",
    "    output_summary = chain.run(split_docs[i:i+5])\n",
    "    wrapped_text = textwrap.fill(output_summary, width=80)\n",
    "    keyword.append(wrapped_text)\n",
    "    #print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create set of keywords\n",
    "Keywords_list=[''.join(keyword[0:len(keyword)])]\n",
    "Keywords_array=str(Keywords_list[0]).split(\",\")\n",
    "Keywords_set=set(Keywords_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4989"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Keywords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_text: 26488 tokens_max_template: 99\n",
      "200\n",
      "Keywords generation successful!!\n",
      "Most Frequently Used Keywords: \n",
      "1. Tax\n",
      "2. Revenue\n",
      "3. Department\n",
      "4. Sales\n",
      "5. Use Tax\n",
      "6. Motor Fuel\n",
      "7. Diesel Fuel\n",
      "8. Exemptions\n",
      "9. Administration\n",
      "10. Payment\n",
      "11. Interest\n",
      "12. Penalty\n",
      "13. State\n",
      "14. Local\n",
      "15. Rate\n",
      "16. Allocation\n",
      "17. Act\n",
      "18. Property\n",
      "19. License\n",
      "20. Report\n",
      "21. Return\n",
      "22. Taxpayer\n",
      "23. Collection\n",
      "24. Records\n",
      "25. Refund\n",
      "26. Fuel\n",
      "27. Gasoline\n",
      "28. Tribunal\n",
      "29. Court\n",
      "30. Appeal\n",
      "31. Michigan\n",
      "32. Texas\n",
      "33. Wyoming\n",
      "34. Georgia\n",
      "35. Florida\n",
      "36. Illinois\n",
      "37. Act 327\n",
      "38. Act 186\n",
      "39. Act 174\n",
      "40. Act 94\n",
      "41. Act 122\n",
      "42. Act 95\n",
      "43. Act 164\n",
      "44. Act 458\n",
      "45. Act 188\n",
      "46. Act 325\n",
      "47. Act 298\n",
      "48. Act 86\n",
      "49. Act 102\n",
      "50. Act 171\n"
     ]
    }
   ],
   "source": [
    "#Pass the keywords generated by langchain to generic and get the final keywords\n",
    "output_final=str()\n",
    "for i in range(0,1):\n",
    "    text=text1\n",
    "    template=\"Find the most frequently used keywords in the following text. Give the output in python list format\"\n",
    "    output=generic_model(text, template, temperature =0, max_tokens = output_max_tokens(text, template))\n",
    "    output_final=output_final+ output\n",
    "print(output_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list of the keywords generated by Generic model(Final Keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/nMost', 'Frequently', 'Used', 'Keywords:', '\\n\\n1.', 'Tax\\n2.', 'Revenue\\n3.', 'Department\\n4.', 'Sales\\n5.', 'Use', 'Tax\\n6.', 'Motor', 'Fuel\\n7.', 'Diesel\\n8.', 'Exemptions\\n9.', 'Administration\\n10.', 'Rate\\n11.', 'State\\n12.', 'Local\\n13.', 'Interest\\n14.', 'Penalty\\n15.', 'Act\\n16.', 'Allocation\\n17.', 'Collection\\n18.', 'Payment\\n19.', 'Return\\n20.', 'Report\\n21.', 'License\\n22.', 'Property\\n23.', 'Tribunal\\n24.', 'Refund\\n25.', 'Credit\\n26.', 'Fuel\\n27.', 'Gasoline\\n28.', 'Taxpayer\\n29.', 'Records\\n30.', 'Agreement\\n31.', 'Taxable\\n32.', 'Exemption\\n33.', 'Michigan\\n34.', 'Wyoming\\n35.', 'GHG\\n36.', 'Inventory\\n37.', 'CCS\\n38.', 'Environmental', 'Quality\\n39.', 'State', 'Treasurer\\n40.', 'Department', 'of', 'Revenue\\n41.', 'Sales', 'Tax\\n42.', 'Use', 'Tax', 'Act\\n43.', 'State', 'Tax', 'Commission\\n44.', 'Administrative', 'Procedures', 'Act\\n45.', 'Tax', 'Tribunal\\n46.', 'Tax', 'Rate\\n47.', 'Tax', 'Collection\\n48.', 'Tax', 'Law\\n49.', 'Taxable', 'Miles\\n50.', 'Taxable', 'Gallons']\n"
     ]
    }
   ],
   "source": [
    "#Split the wordson space\n",
    "output=output_final.split(\" \")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all the special characters and spaces before and after the words\n",
    "import re\n",
    "kws = ['/nMost', 'Frequently', 'Used', 'Keywords:', '\\n\\n1.', 'Tax\\n2.', 'Revenue\\n3.', 'Department\\n4.', 'Sales\\n5.', 'Use', 'Tax\\n6.', 'Motor', 'Fuel\\n7.', 'Diesel\\n8.', 'Exemptions\\n9.', 'Administration\\n10.', 'Rate\\n11.', 'State\\n12.', 'Local\\n13.', 'Interest\\n14.', 'Penalty\\n15.', 'Act\\n16.', 'Allocation\\n17.', 'Collection\\n18.', 'Payment\\n19.', 'Return\\n20.', 'Report\\n21.', 'License\\n22.', 'Property\\n23.', 'Tribunal\\n24.', 'Refund\\n25.', 'Credit\\n26.', 'Fuel\\n27.', 'Gasoline\\n28.', 'Taxpayer\\n29.', 'Records\\n30.', 'Agreement\\n31.', 'Taxable\\n32.', 'Exemption\\n33.', 'Michigan\\n34.', 'Wyoming\\n35.', 'GHG\\n36.', 'Inventory\\n37.', 'CCS\\n38.', 'Environmental', 'Quality\\n39.', 'State', 'Treasurer\\n40.', 'Department', 'of', 'Revenue\\n41.', 'Sales', 'Tax\\n42.', 'Use', 'Tax', 'Act\\n43.', 'State', 'Tax', 'Commission\\n44.', 'Administrative', 'Procedures', 'Act\\n45.', 'Tax', 'Tribunal\\n46.', 'Tax', 'Rate\\n47.', 'Tax', 'Collection\\n48.', 'Tax', 'Law\\n49.', 'Taxable', 'Miles\\n50.', 'Taxable', 'Gallons']\n",
    "\n",
    "keywords_final= []\n",
    "for word in kws:\n",
    "    word = word.replace('\\n', '')\n",
    "    word = word.replace('/n', '')\n",
    "    word = re.sub('\\d+\\.', '', word)\n",
    "    keywords_final.append(word)\n",
    "\n",
    "keywords_final=keywords_final[5:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tax', 'Revenue', 'Department', 'Use', 'Tax', 'Motor', 'Fuel', 'Diesel', 'Exemptions', 'Administration', 'Rate', 'State', 'Local', 'Interest', 'Penalty', 'Act', 'Allocation', 'Collection', 'Payment', 'Return', 'Report', 'License', 'Property', 'Tribunal', 'Refund', 'Credit', 'Fuel', 'Gasoline', 'Taxpayer', 'Records', 'Agreement', 'Taxable', 'Exemption', 'Michigan', 'Wyoming', 'GHG', 'Inventory', 'CCS', 'Environmental', 'Quality', 'State', 'Treasurer', 'Department', 'of']\n"
     ]
    }
   ],
   "source": [
    "# keywords_final.remove('Sales')\n",
    "print(keywords_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a vector similarity search\n",
    " \n",
    "Execute a pure vector similarity search using the similarity_search() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "45\n",
      "(b) A sale of tangible personal property not for resale to a regularly organized church or house of religiousworship, except the following:\n",
      "(\n",
      "i) Sales in activities that are mainly commercial enterprises.(\n",
      "ii) Sales of vehicles licensed for use on public highways other than a passenger van or bus with amanufacturer's rated seating capacity of 10 or more that is used primarily for the transportation of individuals\n",
      "for religious purposes.\n",
      "(c) The sale of food to bona fide enrolled students by a school or other educational institution not operated\n",
      "for profit.\n",
      "(d) The sale of a vessel designated for commercial use of registered tonnage of 500 tons or more, if\n",
      "produced upon special order of the purchaser, and bunker and galley fuel, provisions, supplies, maintenance,\n",
      "and repairs for the exclusive use of the vessel engaged in interstate commerce.\n",
      "(e) Except as otherwise provided under subsection (3), a sale of tangible personal property to a person\n",
      "engaged in a business enterprise that uses or consumes the tangible personal property, directly or indirectly,\n",
      "for either the tilling, planting, draining, caring for, maintaining, or harvesting of things of the soil or the\n",
      "breeding, raising, or caring for livestock, poultry, or horticultural products, including the transfers of\n",
      "livestock, poultry, or horticultural products for further growth.\n",
      "(f) Except as otherwise provided under subsection (3), a sale of any of the following to a person engaged in\n"
     ]
    }
   ],
   "source": [
    "# Find the relavant chunks of data from the pdf's for all the keywords\n",
    "# Perform a similarity search\n",
    "keyword_info=[]\n",
    "print(len(keywords_final))\n",
    "for i in range(0,len(keywords_final)):\n",
    "    docs = vector_store.similarity_search(query=str(keywords_final[i]), k=1, search_type='similarity')\n",
    "#print(docs[0].page_content)\n",
    "    keyword_info.append(str(docs[0].page_content))\n",
    "#print(str(keywords_final[1]))    \n",
    "print(len(keyword_info))  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Definitions for all the keywords using Generic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "tokens_text: 33 tokens_max_template: 67\n",
      "200\n",
      "Keywords generation successful!!\n",
      "17\n",
      "tokens_text: 1 tokens_max_template: 67\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "18\n",
      "tokens_text: 953 tokens_max_template: 64\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "19\n",
      "tokens_text: 1 tokens_max_template: 63\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "20\n",
      "tokens_text: 1 tokens_max_template: 63\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "21\n",
      "tokens_text: 777 tokens_max_template: 64\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "22\n",
      "tokens_text: 1 tokens_max_template: 65\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "23\n",
      "tokens_text: 886 tokens_max_template: 65\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "24\n",
      "tokens_text: 83 tokens_max_template: 63\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "25\n",
      "tokens_text: 974 tokens_max_template: 63\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "26\n",
      "tokens_text: 13 tokens_max_template: 61\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "27\n",
      "tokens_text: 50 tokens_max_template: 65\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "28\n",
      "tokens_text: 202 tokens_max_template: 65\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "29\n",
      "tokens_text: 679 tokens_max_template: 64\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "30\n",
      "tokens_text: 322 tokens_max_template: 66\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "429\n",
      "'choices'\n",
      "Keywords generation successful!!\n",
      "31\n",
      "tokens_text: 148 tokens_max_template: 64\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "32\n",
      "tokens_text: 396 tokens_max_template: 66\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "33\n",
      "tokens_text: 9 tokens_max_template: 65\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "34\n",
      "tokens_text: 25 tokens_max_template: 64\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "429\n",
      "'choices'\n",
      "Keywords generation successful!!\n",
      "35\n",
      "tokens_text: 1 tokens_max_template: 60\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "36\n",
      "tokens_text: 679 tokens_max_template: 66\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "37\n",
      "tokens_text: 319 tokens_max_template: 60\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "38\n",
      "tokens_text: 1 tokens_max_template: 70\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "39\n",
      "tokens_text: 1 tokens_max_template: 64\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "40\n",
      "tokens_text: 1 tokens_max_template: 62\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "41\n",
      "tokens_text: 366 tokens_max_template: 66\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "42\n",
      "tokens_text: 1 tokens_max_template: 67\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "43\n",
      "tokens_text: 1 tokens_max_template: 59\n",
      "429\n",
      "ERRORED OUT :(\n",
      "\n",
      " 'choices'\n",
      "retrying...\n",
      "200\n",
      "Keywords generation successful!!\n",
      "44\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [277]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m      5\u001b[0m output_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m()\n\u001b[1;32m----> 7\u001b[0m template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefine the word \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(keywords_final[i])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in the context of the given documents: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(keyword_definitions[i])\n\u001b[0;32m      9\u001b[0m output\u001b[38;5;241m=\u001b[39mgeneric_model(text, template, temperature \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, max_tokens \u001b[38;5;241m=\u001b[39m output_max_tokens(text, template))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "final_list=[]\n",
    "for i in range(0,len(keyword_info)):\n",
    "    \n",
    "    print(i)\n",
    "    output_final=str()\n",
    "    \n",
    "    template=f\"Define the word {str(keywords_final[i])} in the context of the given documents: \\n\"\n",
    "    text=str(keyword_info[i])\n",
    "    output=generic_model(text, template, temperature =0, max_tokens = output_max_tokens(text, template))\n",
    "    final_list.append(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the definitions to a CSV file\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(final_list)\n",
    "df.to_csv('C:/Users/Divya.Koyyalamudi/Downloads/prolicious-et-dossier/prolicious-et-dossier/Keyword_definitions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_text: 886 tokens_max_template: 60\n",
      "200\n",
      "Keywords generation successful!!\n",
      "In the context of the given documents, the word \"tax\" refers to a compulsory financial charge or levy imposed by the government on individuals, businesses, or other entities to fund government spending and public services. Examples of taxes mentioned in the document include Motor Vehicle Sales and Use tax, Franchise tax, Insurance Premiums taxes, and Hotel Occupancy taxes.\n"
     ]
    }
   ],
   "source": [
    "# output_final=str()\n",
    "# template=f\"Define the word {str(keywords_final[1])} in the context of the given documents: \\n\"\n",
    "# text=str(keyword_definitions[0])\n",
    "# # print(template,text)\n",
    "# output=generic_model(text, template, temperature =0, max_tokens = output_max_tokens(text, template))\n",
    "# # output_final=output_final+ output\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Definitions for all the keywords using Langchain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the text into paragarahs and convert it into documents\n",
    "import nltk\n",
    "paragraphs = [sent for sent in nltk.sent_tokenize(text) if sent]\n",
    "para_doc = text_splitter.create_documents(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap\n",
    "\n",
    "#customize the prompt\n",
    "\n",
    "map_prompt_template =  \"Define the word\" +str(keywords_final[1]) + \"in the context of the given documents.Input:{text}\"\n",
    "combine_prompt_template = \"You are a copy editor. Combine the context.Input:{text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tax: It is used to generate revenue for the government and to fund public services.\n"
     ]
    }
   ],
   "source": [
    "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "chain = load_summarize_chain(llm, chain_type='map_reduce', map_prompt=map_prompt, combine_prompt=combine_prompt, verbose=False)\n",
    "output_summary = chain.run(para_doc)\n",
    "wrapped_text = textwrap.fill(output_summary, width=80)\n",
    "print(str(keywords_final[1])+\":\"+wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "645053d6307d413a1a75681b5ebb6449bb2babba4bcb0bf65a1ddc3dbefb108a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
